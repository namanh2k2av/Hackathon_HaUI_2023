{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Aoa2NzIlxqJ",
        "outputId": "c59e4867-188a-45db-ed0c-d5a7210f6924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m0cFFyvjnmC",
        "outputId": "0ad0af5e-a9db-454f-e0c6-05c5754e9025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/videos\n",
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1Czz-WVEk5xo4-8CsRGOk0z8Tl5yiQcKS&export=download\n",
            "To: /content/videos/Video_Phase2.zip\n",
            "100% 446M/446M [00:04<00:00, 94.8MB/s]\n",
            "Archive:  Video_Phase2.zip\n",
            "  inflating: Doto_102.mp4            \n",
            "  inflating: Doto_104.mp4            \n",
            "  inflating: Doto_105.mp4            \n",
            "  inflating: Doto_106.mp4            \n",
            "  inflating: Doto_107.mp4            \n",
            "  inflating: Doto_111.mp4            \n",
            "  inflating: Doto_112.mp4            \n",
            "  inflating: Doto_113.mp4            \n",
            "  inflating: Doto_114.mp4            \n",
            "  inflating: Doto_115.mp4            \n",
            "  inflating: Doto_131.mp4            \n",
            "  inflating: Doto_133.mp4            \n",
            "  inflating: Doto_135.mp4            \n",
            "  inflating: Doto_136.mp4            \n",
            "  inflating: Kabu_0.mp4              \n",
            "  inflating: Kabu_10.mp4             \n",
            "  inflating: Kabu_11.mp4             \n",
            "  inflating: Kabu_13.mp4             \n",
            "  inflating: Kabu_14.mp4             \n",
            "  inflating: Kabu_15.mp4             \n",
            "  inflating: Kabu_16.mp4             \n",
            "  inflating: Kabu_17.mp4             \n",
            "  inflating: Kabu_18.mp4             \n",
            "  inflating: Kabu_2.mp4              \n",
            "  inflating: Kabu_40.mp4             \n",
            "  inflating: Kabu_4.mp4              \n",
            "  inflating: Kabu_5.mp4              \n",
            "  inflating: Kabu_61.mp4             \n",
            "  inflating: Kabu_6.mp4              \n",
            "  inflating: Kabu_71.mp4             \n",
            "  inflating: Kabu_72.mp4             \n",
            "  inflating: Kabu_73.mp4             \n",
            "  inflating: Kabu_74.mp4             \n",
            "  inflating: Kabu_76.mp4             \n",
            "  inflating: Kabu_77.mp4             \n",
            "  inflating: Kabu_7.mp4              \n",
            "  inflating: Kabu_80.mp4             \n",
            "  inflating: Kabu_81.mp4             \n",
            "  inflating: Kabu_85.mp4             \n",
            "  inflating: Kabu_86.mp4             \n",
            "  inflating: Kabu_87.mp4             \n",
            "  inflating: Kabu_89.mp4             \n",
            "  inflating: Kabu_90.mp4             \n",
            "  inflating: Kabu_91.mp4             \n",
            "  inflating: Kabu_93.mp4             \n",
            "  inflating: Kabu_96.mp4             \n",
            "  inflating: Kabu_98.mp4             \n",
            "  inflating: Kabu_99.mp4             \n"
          ]
        }
      ],
      "source": [
        "!mkdir {HOME}/videos\n",
        "%cd {HOME}/videos\n",
        "!gdown 'https://drive.google.com/u/0/uc?id=1Czz-WVEk5xo4-8CsRGOk0z8Tl5yiQcKS&export=download'\n",
        "!unzip 'Video_Phase2.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqYEZpaspF9t",
        "outputId": "3ba15601-345d-4116-81c2-11461850cd3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yolox.__version__: 0.1.0\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "!git clone https://github.com/ifzhang/ByteTrack.git\n",
        "%cd {HOME}/ByteTrack\n",
        "\n",
        "# workaround related to https://github.com/roboflow/notebooks/issues/80\n",
        "!sed -i 's/onnx==1.8.1/onnx==1.9.0/g' requirements.txt\n",
        "\n",
        "!pip3 install -q -r requirements.txt\n",
        "!python3 setup.py -q develop\n",
        "!pip install -q cython_bbox\n",
        "!pip install -q onemetric\n",
        "# workaround related to https://github.com/roboflow/notebooks/issues/112 and https://github.com/roboflow/notebooks/issues/106\n",
        "!pip install -q loguru lap thop\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import sys\n",
        "sys.path.append(f\"{HOME}/ByteTrack\")\n",
        "\n",
        "import yolox\n",
        "print(\"yolox.__version__:\", yolox.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download pretrained model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXarS5UQ4deB",
        "outputId": "25ee792d-f7b8-442d-b6af-c234d9d2b391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1HX2_JpMOjOIj1Z9rJjoet9XNy_cCAs5U&export=download\n",
            "To: /content/ByteTrack/bytetrack_x_mot20.tar\n",
            "100% 793M/793M [00:08<00:00, 91.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# == Download pretrained X model weights ==\n",
        "!gdown 'https://drive.google.com/u/0/uc?id=1HX2_JpMOjOIj1Z9rJjoet9XNy_cCAs5U&export=download'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PUiju0LDjbj",
        "outputId": "12485dc5-0506-4904-fa38-31f762543303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cncR3LmIOlZlGTUV6LeipW8og4qb0WLS\n",
            "To: /content/ByteTrack/NewAttVGG.pt\n",
            "100% 60.9M/60.9M [00:00<00:00, 74.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1cncR3LmIOlZlGTUV6LeipW8og4qb0WLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tv_H2L3lwCV",
        "outputId": "7844d781-d6e3-4f65-aa5e-a1878dc8065e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5\n",
            "To: /content/ByteTrack/bytetrack_x_mot17.pth.tar\n",
            "100% 793M/793M [00:08<00:00, 93.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id \"1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Táº¡o txt submit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdHlrFynLgE4"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import os.path as osp\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import keras\n",
        "from loguru import logger\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from yolox.data.data_augment import preproc\n",
        "from yolox.exp import get_exp\n",
        "from yolox.utils import fuse_model, get_model_info, postprocess\n",
        "from yolox.utils.visualize import plot_tracking\n",
        "from yolox.tracker.byte_tracker import BYTETracker\n",
        "from yolox.tracking_utils.timer import Timer\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "\n",
        "IMAGE_EXT = [\".jpg\", \".jpeg\", \".webp\", \".bmp\", \".png\"]\n",
        "\n",
        "\n",
        "def make_parser():\n",
        "    parser = argparse.ArgumentParser(\"ByteTrack Demo!\")\n",
        "    parser.add_argument(\n",
        "        \"--demo\", default=\"image\", help=\"demo type, eg. image, video and webcam\"\n",
        "    )\n",
        "    parser.add_argument(\"-expn\", \"--experiment-name\", type=str, default=None)\n",
        "    parser.add_argument(\"-n\", \"--name\", type=str, default=None, help=\"model name\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        #\"--path\", default=\"./datasets/mot/train/MOT17-05-FRCNN/img1\", help=\"path to images or video\"\n",
        "        \"--path\", default=\"./videos/palace.mp4\", help=\"path to images or video\"\n",
        "    )\n",
        "    parser.add_argument(\"--camid\", type=int, default=0, help=\"webcam demo camera id\")\n",
        "    parser.add_argument(\n",
        "        \"--save_result\",\n",
        "        action=\"store_true\",\n",
        "        help=\"whether to save the inference result of image/video\",\n",
        "    )\n",
        "\n",
        "    # exp file\n",
        "    parser.add_argument(\n",
        "        \"-f\",\n",
        "        \"--exp_file\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"pls input your expriment description file\",\n",
        "    )\n",
        "    parser.add_argument(\"-c\", \"--ckpt\", default=None, type=str, help=\"ckpt for eval\")\n",
        "    parser.add_argument(\n",
        "        \"--device\",\n",
        "        default=\"gpu\",\n",
        "        type=str,\n",
        "        help=\"device to run our model, can either be cpu or gpu\",\n",
        "    )\n",
        "    parser.add_argument(\"--conf\", default=None, type=float, help=\"test conf\")\n",
        "    parser.add_argument(\"--nms\", default=None, type=float, help=\"test nms threshold\")\n",
        "    parser.add_argument(\"--tsize\", default=None, type=int, help=\"test img size\")\n",
        "    parser.add_argument(\"--fps\", default=30, type=int, help=\"frame rate (fps)\")\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        dest=\"fp16\",\n",
        "        default=False,\n",
        "        action=\"store_true\",\n",
        "        help=\"Adopting mix precision evaluating.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fuse\",\n",
        "        dest=\"fuse\",\n",
        "        default=False,\n",
        "        action=\"store_true\",\n",
        "        help=\"Fuse conv and bn for testing.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--trt\",\n",
        "        dest=\"trt\",\n",
        "        default=False,\n",
        "        action=\"store_true\",\n",
        "        help=\"Using TensorRT model for testing.\",\n",
        "    )\n",
        "    # tracking args\n",
        "    parser.add_argument(\"--track_thresh\", type=float, default=0.5, help=\"tracking confidence threshold\")\n",
        "    parser.add_argument(\"--track_buffer\", type=int, default=30, help=\"the frames for keep lost tracks\")\n",
        "    parser.add_argument(\"--match_thresh\", type=float, default=0.8, help=\"matching threshold for tracking\")\n",
        "    parser.add_argument(\n",
        "        \"--aspect_ratio_thresh\", type=float, default=1.6,\n",
        "        help=\"threshold for filtering out boxes of which aspect ratio are above the given value.\"\n",
        "    )\n",
        "    parser.add_argument('--min_box_area', type=float, default=10, help='filter out tiny boxes')\n",
        "    parser.add_argument(\"--mot20\", dest=\"mot20\", default=False, action=\"store_true\", help=\"test mot20.\")\n",
        "    return parser\n",
        "\n",
        "class Predictor(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        exp,\n",
        "        trt_file=None,\n",
        "        decoder=None,\n",
        "        device=torch.device(\"cpu\"),\n",
        "        fp16=False\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.decoder = decoder\n",
        "        self.num_classes = exp.num_classes\n",
        "        self.confthre = exp.test_conf\n",
        "        self.nmsthre = exp.nmsthre\n",
        "        self.test_size = exp.test_size\n",
        "        self.device = device\n",
        "        self.fp16 = fp16\n",
        "        if trt_file is not None:\n",
        "            from torch2trt import TRTModule\n",
        "\n",
        "            model_trt = TRTModule()\n",
        "            model_trt.load_state_dict(torch.load(trt_file))\n",
        "\n",
        "            x = torch.ones((1, 3, exp.test_size[0], exp.test_size[1]), device=device)\n",
        "            self.model(x)\n",
        "            self.model = model_trt\n",
        "        self.rgb_means = (0.485, 0.456, 0.406)\n",
        "        self.std = (0.229, 0.224, 0.225)\n",
        "\n",
        "    def inference(self, img):\n",
        "        img_info = {\"id\": 0}\n",
        "        if isinstance(img, str):\n",
        "            img_info[\"file_name\"] = osp.basename(img)\n",
        "            img = cv2.imread(img)\n",
        "        else:\n",
        "            img_info[\"file_name\"] = None\n",
        "\n",
        "        height, width = img.shape[:2]\n",
        "        img_info[\"height\"] = height\n",
        "        img_info[\"width\"] = width\n",
        "        img_info[\"raw_img\"] = img\n",
        "\n",
        "        img, ratio = preproc(img, self.test_size, self.rgb_means, self.std)\n",
        "        img_info[\"ratio\"] = ratio\n",
        "        img = torch.from_numpy(img).unsqueeze(0).float().to(self.device)\n",
        "        if self.fp16:\n",
        "            img = img.half()  # to FP16\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(img)\n",
        "            if self.decoder is not None:\n",
        "                outputs = self.decoder(outputs, dtype=outputs.type())\n",
        "            outputs = postprocess(\n",
        "                outputs, self.num_classes, self.confthre, self.nmsthre\n",
        "            )\n",
        "            #logger.info(\"Infer time: {:.4f}s\".format(time.time() - t0))\n",
        "        return outputs, img_info\n",
        "\n",
        "\n",
        "def imageflow_demo(predictor, vis_folder, current_time, args):\n",
        "    # load model age gen\n",
        "    # model = MNet()\n",
        "    model = torch.jit.load('/content/ByteTrack/NewAttVGG.pt')\n",
        "    model.eval()\n",
        "    # model_age_gender = keras.models.load_model('/content/model_age_gender')\n",
        "\n",
        "\n",
        "    cap = cv2.VideoCapture(args.path if args.demo == \"video\" else args.camid)\n",
        "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)  # float\n",
        "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    timestamp = time.strftime(\"%Y_%m_%d_%H_%M_%S\", current_time)\n",
        "    save_folder = osp.join(vis_folder, timestamp)\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "    string = args.path.split(\"/\")[-1]\n",
        "    name_file = string.split('.')[0]\n",
        "    print('file: ',name_file)\n",
        "\n",
        "    if args.demo == \"video\":\n",
        "        save_path = osp.join(save_folder, args.path.split(\"/\")[-1])\n",
        "    else:\n",
        "        save_path = osp.join(save_folder, \"camera.mp4\")\n",
        "    # logger.info(f\"video save_path is {save_path}\")\n",
        "    vid_writer = cv2.VideoWriter(\n",
        "        save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (int(width), int(height))\n",
        "    )\n",
        "    tracker = BYTETracker(args, frame_rate=30)\n",
        "    frame_id = 0\n",
        "    results = []\n",
        "\n",
        "    tid_age_info_dict = defaultdict(list)\n",
        "    tid_gender_info_dict = defaultdict(list)\n",
        "    frame_history_age = 20\n",
        "    frame_history_gender = 20\n",
        "\n",
        "    pbar = tqdm(total=None, position=0, leave=True)\n",
        "    try:\n",
        "      while True:\n",
        "          # if frame_id % 20 == 0:\n",
        "          #     print('Processing frame {}'.format(frame_id))\n",
        "          ret_val, frame = cap.read()\n",
        "          if ret_val:\n",
        "              outputs, img_info = predictor.inference(frame)\n",
        "              if outputs[0] is not None:\n",
        "                  online_targets = tracker.update(outputs[0], [img_info['height'], img_info['width']], exp.test_size)\n",
        "\n",
        "                  for t in online_targets:\n",
        "                      tlwh = t.tlwh\n",
        "                      tid = t.track_id\n",
        "                      vertical = tlwh[2] / tlwh[3] > args.aspect_ratio_thresh\n",
        "                      if tlwh[2] * tlwh[3] > args.min_box_area and not vertical:\n",
        "                          if tlwh[0] < 0:\n",
        "                              tlwh[2] += tlwh[0]\n",
        "                              tlwh[0] = 0\n",
        "                          if tlwh[1] < 0:\n",
        "                              tlwh[3] += tlwh[1]\n",
        "                              tlwh[1]\n",
        "\n",
        "                          x, y, width, height = tlwh.astype(int)\n",
        "                          img = frame[y:y+height,x:x+width] / 255.0\n",
        "\n",
        "\n",
        "                          img = img.astype(np.float32)\n",
        "                          img = cv2.resize(img, (112, 224))\n",
        "\n",
        "\n",
        "                          xb = transforms.ToTensor()(img).unsqueeze(0).to('cuda')\n",
        "                          res, _ ,_ = model(xb)\n",
        "\n",
        "                          res = res.cpu().detach().numpy()\n",
        "                          age = np.argmax(res[0][1:])\n",
        "                          gender =  1 if res[0][0] > 0.5 else 0\n",
        "\n",
        "                          tid_age_info_dict[tid].append(age)\n",
        "                          tid_gender_info_dict[tid].append(gender)\n",
        "                          print(f'age{tid}: {tid_age_info_dict[tid]}')\n",
        "                          print(f'gen{tid}: {tid_gender_info_dict[tid]}')\n",
        "\n",
        "                          # Giá»¯ tá»« Äiá»n cÃ³ Ã­t hÆ¡n frame_history entries\n",
        "                          if len(tid_age_info_dict[tid]) > frame_history_age:\n",
        "                            tid_age_info_dict[tid].pop(0)\n",
        "                          if len(tid_gender_info_dict[tid]) > frame_history_gender:\n",
        "                            tid_gender_info_dict[tid].pop(0)\n",
        "\n",
        "                          # Láº¥y giÃ¡ trá» xuáº¥t hiá»n nhiá»u nháº¥t trong 20 frame\n",
        "                          most_common_age = max(tid_age_info_dict[tid], key=tid_age_info_dict[tid].count)\n",
        "                          most_common_gender = max(tid_gender_info_dict[tid], key=tid_gender_info_dict[tid].count)\n",
        "\n",
        "                          results.append(\n",
        "                              f\"{frame_id+1},{tid},{tlwh[0]:.2f},{tlwh[1]:.2f},{tlwh[2]:.2f},{tlwh[3]:.2f},{most_common_gender},{most_common_age}\\n\"\n",
        "                          )\n",
        "\n",
        "              ch = cv2.waitKey(1)\n",
        "              if ch == 27 or ch == ord(\"q\") or ch == ord(\"Q\"):\n",
        "                  break\n",
        "          else:\n",
        "              break\n",
        "          frame_id += 1\n",
        "          pbar.update(1)\n",
        "    finally:\n",
        "      pbar.close()\n",
        "\n",
        "\n",
        "    if args.save_result:\n",
        "        res_file = osp.join(vis_folder, f\"{name_file}.txt\")\n",
        "        with open(res_file, 'w') as f:\n",
        "            f.writelines(results)\n",
        "        logger.info(f\"save results to {res_file}\")\n",
        "\n",
        "\n",
        "def main(exp, args):\n",
        "    # ÄÆ°á»ng dáº«n video\n",
        "    link_path = [\n",
        "        '/content/videos/Doto_102.mp4',\n",
        "        '/content/videos/Doto_104.mp4',\n",
        "        '/content/videos/Doto_105.mp4',\n",
        "        '/content/videos/Doto_106.mp4',\n",
        "        '/content/videos/Doto_107.mp4',\n",
        "        '/content/videos/Doto_111.mp4',\n",
        "        '/content/videos/Doto_112.mp4',\n",
        "        '/content/videos/Doto_113.mp4',\n",
        "        '/content/videos/Doto_114.mp4',\n",
        "        '/content/videos/Doto_115.mp4',\n",
        "        '/content/videos/Doto_131.mp4',\n",
        "        '/content/videos/Doto_133.mp4',\n",
        "        '/content/videos/Doto_135.mp4',\n",
        "        '/content/videos/Doto_136.mp4',\n",
        "        '/content/videos/Kabu_0.mp4',\n",
        "        '/content/videos/Kabu_2.mp4'\n",
        "    ]\n",
        "\n",
        "    if not args.experiment_name:\n",
        "        args.experiment_name = exp.exp_name\n",
        "\n",
        "    output_dir = osp.join(exp.output_dir, args.experiment_name)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    if args.save_result:\n",
        "        vis_folder = osp.join(output_dir, \"track_vis\")\n",
        "        os.makedirs(vis_folder, exist_ok=True)\n",
        "\n",
        "    if args.trt:\n",
        "        args.device = \"gpu\"\n",
        "    args.device = torch.device(\"cuda\" if args.device == \"gpu\" else \"cpu\")\n",
        "\n",
        "    logger.info(\"Args: {}\".format(args))\n",
        "\n",
        "    if args.conf is not None:\n",
        "        exp.test_conf = args.conf\n",
        "    if args.nms is not None:\n",
        "        exp.nmsthre = args.nms\n",
        "    if args.tsize is not None:\n",
        "        exp.test_size = (args.tsize, args.tsize)\n",
        "\n",
        "    model = exp.get_model().to(args.device)\n",
        "    print(\"Model Summary: {}\".format(get_model_info(model, exp.test_size)))\n",
        "    model.eval()\n",
        "\n",
        "    if not args.trt:\n",
        "        if args.ckpt is None:\n",
        "            ckpt_file = osp.join(output_dir, \"best_ckpt.pth.tar\")\n",
        "        else:\n",
        "            ckpt_file = args.ckpt\n",
        "        ckpt = torch.load(ckpt_file, map_location=\"cpu\")\n",
        "        # load the model state dict\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "\n",
        "    if args.fuse:\n",
        "        model = fuse_model(model)\n",
        "\n",
        "    if args.fp16:\n",
        "        model = model.half()  # to FP16\n",
        "\n",
        "    if args.trt:\n",
        "        assert not args.fuse, \"TensorRT model is not support model fusing!\"\n",
        "        trt_file = osp.join(output_dir, \"model_trt.pth\")\n",
        "        assert osp.exists(\n",
        "            trt_file\n",
        "        ), \"TensorRT model is not found!\\n Run python3 tools/trt.py first!\"\n",
        "        model.head.decode_in_inference = False\n",
        "        decoder = model.head.decode_outputs\n",
        "    else:\n",
        "        trt_file = None\n",
        "        decoder = None\n",
        "\n",
        "    for path in link_path:\n",
        "      args.path = path\n",
        "      predictor = Predictor(model, exp, trt_file, decoder, args.device, args.fp16)\n",
        "      current_time = time.localtime()\n",
        "      imageflow_demo(predictor, vis_folder, current_time, args)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = make_parser().parse_args()\n",
        "\n",
        "    args.demo = 'video'\n",
        "\n",
        "    # args.exp_file = 'exps/example/mot/yolox_x_mix_mot20_ch.py'\n",
        "    # args.ckpt = '/content/ByteTrack/bytetrack_x_mot20.tar'\n",
        "    args.exp_file = 'exps/example/mot/yolox_x_mix_det.py'\n",
        "    args.ckpt = '/content/ByteTrack/bytetrack_x_mot17.pth.tar'\n",
        "\n",
        "\n",
        "\n",
        "    args.fp16 = True\n",
        "    args.fuse = True\n",
        "    args.save_result = True\n",
        "    args.mot20 = True\n",
        "\n",
        "    exp = get_exp(args.exp_file, args.name)\n",
        "\n",
        "    main(exp, args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdIiT8D5KUKi"
      },
      "source": [
        "Táº¡o Video Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggvTk8PBVlFz"
      },
      "outputs": [],
      "source": [
        "def plot_tracking(image, tlwhs, obj_ids, frame_ids=0, fps=0., ids2=None, ages=None, genders=None):\n",
        "    im = np.ascontiguousarray(np.copy(image))\n",
        "    im_h, im_w = im.shape[:2]\n",
        "\n",
        "    text_scale = 2\n",
        "    text_thickness = 2\n",
        "    line_thickness = 3\n",
        "\n",
        "    radius = max(5, int(im_w/140.))\n",
        "    cv2.putText(im, 'frame: %d fps: %.2f num: %d' % (frame_ids, fps, len(tlwhs)),\n",
        "                (0, int(15 * text_scale)), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), thickness=2)\n",
        "\n",
        "    for i, tlwh in enumerate(tlwhs):\n",
        "        x1, y1, w, h = tlwh\n",
        "        intbox = tuple(map(int, (x1, y1, x1 + w, y1 + h)))\n",
        "        obj_id = int(obj_ids[i])\n",
        "        age = ages[i] if ages is not None else None\n",
        "        gender = genders[i] if genders is not None else None\n",
        "        id_text = '{} A{} G{}'.format(int(obj_id), age, gender)\n",
        "        if ids2 is not None:\n",
        "            id_text = id_text + ', {}'.format(int(ids2[i]))\n",
        "        color = get_color(abs(obj_id))\n",
        "        cv2.rectangle(im, intbox[0:2], intbox[2:4], color=color, thickness=line_thickness)\n",
        "        cv2.putText(im, id_text, (intbox[0], intbox[1]), cv2.FONT_HERSHEY_PLAIN, text_scale, (0, 0, 255),\n",
        "                    thickness=text_thickness)\n",
        "    return im\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft4ITgJM8pvm",
        "outputId": "f1bafd2b-2ac7-40a0-bbe3-40b2c4bceff3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-12-18 03:46:37.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m388\u001b[0m - \u001b[1mArgs: Namespace(demo='video', experiment_name='yolox_x_mix_det', name=None, path='/content/videos/Kabu_0.mp4', camid=0, save_result=True, exp_file='exps/example/mot/yolox_x_mix_det.py', ckpt='/content/ByteTrack/bytetrack_x_mot17.pth.tar', device=device(type='cuda'), conf=None, nms=None, tsize=None, fps=30, fp16=True, fuse=True, trt=False, track_thresh=0.5, track_buffer=30, match_thresh=0.8, aspect_ratio_thresh=1.6, min_box_area=10, mot20=True)\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\u001b[32m2023-12-18 03:46:40.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m398\u001b[0m - \u001b[1mModel Summary: Params: 99.00M, Gflops: 793.21\u001b[0m\n",
            "\u001b[32m2023-12-18 03:46:40.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m406\u001b[0m - \u001b[1mloading checkpoint\u001b[0m\n",
            "\u001b[32m2023-12-18 03:46:40.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m410\u001b[0m - \u001b[1mloaded checkpoint done.\u001b[0m\n",
            "\u001b[32m2023-12-18 03:46:40.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1m\tFusing model...\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:844: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if param.grad is not None:\n",
            "\u001b[32m2023-12-18 03:46:42.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimageflow_demo\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mvideo save_path is ./YOLOX_outputs/yolox_x_mix_det/track_vis/2023_12_18_03_46_41/Kabu_0.mp4\u001b[0m\n",
            "901it [04:22,  3.44it/s]\n",
            "\u001b[32m2023-12-18 03:51:04.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimageflow_demo\u001b[0m:\u001b[36m370\u001b[0m - \u001b[1msave results to ./YOLOX_outputs/yolox_x_mix_det/track_vis/2023_12_18_03_46_41.txt\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import os.path as osp\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import keras\n",
        "from loguru import logger\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from yolox.data.data_augment import preproc\n",
        "from yolox.exp import get_exp\n",
        "from yolox.utils import fuse_model, get_model_info, postprocess\n",
        "from yolox.utils.visualize import get_color\n",
        "from yolox.tracker.byte_tracker import BYTETracker\n",
        "from yolox.tracking_utils.timer import Timer\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "IMAGE_EXT = [\".jpg\", \".jpeg\", \".webp\", \".bmp\", \".png\"]\n",
        "\n",
        "\n",
        "def make_parser():\n",
        "    parser = argparse.ArgumentParser(\"ByteTrack Demo!\")\n",
        "    parser.add_argument(\n",
        "        \"--demo\", default=\"image\", help=\"demo type, eg. image, video and webcam\"\n",
        "    )\n",
        "    parser.add_argument(\"-expn\", \"--experiment-name\", type=str, default=None)\n",
        "    parser.add_argument(\"-n\", \"--name\", type=str, default=None, help=\"model name\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        #\"--path\", default=\"./datasets/mot/train/MOT17-05-FRCNN/img1\", help=\"path to images or video\"\n",
        "        \"--path\", default=\"./videos/palace.mp4\", help=\"path to images or video\"\n",
        "    )\n",
        "    parser.add_argument(\"--camid\", type=int, default=0, help=\"webcam demo camera id\")\n",
        "    parser.add_argument(\n",
        "        \"--save_result\",\n",
        "        action=\"store_true\",\n",
        "        help=\"whether to save the inference result of image/video\",\n",
        "    )\n",
        "\n",
        "    # exp file\n",
        "    parser.add_argument(\n",
        "        \"-f\",\n",
        "        \"--exp_file\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"pls input your expriment description file\",\n",
        "    )\n",
        "    parser.add_argument(\"-c\", \"--ckpt\", default=None, type=str, help=\"ckpt for eval\")\n",
        "    parser.add_argument(\n",
        "        \"--device\",\n",
        "        default=\"gpu\",\n",
        "        type=str,\n",
        "        help=\"device to run our model, can either be cpu or gpu\",\n",
        "    )\n",
        "    parser.add_argument(\"--conf\", default=None, type=float, help=\"test conf\")\n",
        "    parser.add_argument(\"--nms\", default=None, type=float, help=\"test nms threshold\")\n",
        "    parser.add_argument(\"--tsize\", default=None, type=int, help=\"test img size\")\n",
        "    parser.add_argument(\"--fps\", default=30, type=int, help=\"frame rate (fps)\")\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        dest=\"fp16\",\n",
        "        default=False,\n",
        "        action=\"store_true\",\n",
        "        help=\"Adopting mix precision evaluating.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fuse\",\n",
        "        dest=\"fuse\",\n",
        "        default=False,\n",
        "        action=\"store_true\",\n",
        "        help=\"Fuse conv and bn for testing.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--trt\",\n",
        "        dest=\"trt\",\n",
        "        default=False,\n",
        "        action=\"store_true\",\n",
        "        help=\"Using TensorRT model for testing.\",\n",
        "    )\n",
        "    # tracking args\n",
        "    parser.add_argument(\"--track_thresh\", type=float, default=0.5, help=\"tracking confidence threshold\")\n",
        "    parser.add_argument(\"--track_buffer\", type=int, default=30, help=\"the frames for keep lost tracks\")\n",
        "    parser.add_argument(\"--match_thresh\", type=float, default=0.8, help=\"matching threshold for tracking\")\n",
        "    parser.add_argument(\n",
        "        \"--aspect_ratio_thresh\", type=float, default=1.6,\n",
        "        help=\"threshold for filtering out boxes of which aspect ratio are above the given value.\"\n",
        "    )\n",
        "    parser.add_argument('--min_box_area', type=float, default=10, help='filter out tiny boxes')\n",
        "    parser.add_argument(\"--mot20\", dest=\"mot20\", default=False, action=\"store_true\", help=\"test mot20.\")\n",
        "    return parser\n",
        "\n",
        "\n",
        "def get_image_list(path):\n",
        "    image_names = []\n",
        "    for maindir, subdir, file_name_list in os.walk(path):\n",
        "        for filename in file_name_list:\n",
        "            apath = osp.join(maindir, filename)\n",
        "            ext = osp.splitext(apath)[1]\n",
        "            if ext in IMAGE_EXT:\n",
        "                image_names.append(apath)\n",
        "    return image_names\n",
        "\n",
        "\n",
        "def write_results(filename, results):\n",
        "    save_format = '{frame},{id},{x1},{y1},{w},{h},{s},-1,-1,-1\\n'\n",
        "    with open(filename, 'w') as f:\n",
        "        for frame_id, tlwhs, track_ids, scores in results:\n",
        "            for tlwh, track_id, score in zip(tlwhs, track_ids, scores):\n",
        "                if track_id < 0:\n",
        "                    continue\n",
        "                x1, y1, w, h = tlwh\n",
        "                line = save_format.format(frame=frame_id, id=track_id, x1=round(x1, 1), y1=round(y1, 1), w=round(w, 1), h=round(h, 1), s=round(score, 2))\n",
        "                f.write(line)\n",
        "    logger.info('save results to {}'.format(filename))\n",
        "\n",
        "\n",
        "class Predictor(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        exp,\n",
        "        trt_file=None,\n",
        "        decoder=None,\n",
        "        device=torch.device(\"cpu\"),\n",
        "        fp16=False\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.decoder = decoder\n",
        "        self.num_classes = exp.num_classes\n",
        "        self.confthre = exp.test_conf\n",
        "        self.nmsthre = exp.nmsthre\n",
        "        self.test_size = exp.test_size\n",
        "        self.device = device\n",
        "        self.fp16 = fp16\n",
        "        if trt_file is not None:\n",
        "            from torch2trt import TRTModule\n",
        "\n",
        "            model_trt = TRTModule()\n",
        "            model_trt.load_state_dict(torch.load(trt_file))\n",
        "\n",
        "            x = torch.ones((1, 3, exp.test_size[0], exp.test_size[1]), device=device)\n",
        "            self.model(x)\n",
        "            self.model = model_trt\n",
        "        self.rgb_means = (0.485, 0.456, 0.406)\n",
        "        self.std = (0.229, 0.224, 0.225)\n",
        "\n",
        "    def inference(self, img, timer):\n",
        "        img_info = {\"id\": 0}\n",
        "        if isinstance(img, str):\n",
        "            img_info[\"file_name\"] = osp.basename(img)\n",
        "            img = cv2.imread(img)\n",
        "        else:\n",
        "            img_info[\"file_name\"] = None\n",
        "\n",
        "        height, width = img.shape[:2]\n",
        "        img_info[\"height\"] = height\n",
        "        img_info[\"width\"] = width\n",
        "        img_info[\"raw_img\"] = img\n",
        "\n",
        "        img, ratio = preproc(img, self.test_size, self.rgb_means, self.std)\n",
        "        img_info[\"ratio\"] = ratio\n",
        "        img = torch.from_numpy(img).unsqueeze(0).float().to(self.device)\n",
        "        if self.fp16:\n",
        "            img = img.half()  # to FP16\n",
        "\n",
        "        with torch.no_grad():\n",
        "            timer.tic()\n",
        "            outputs = self.model(img)\n",
        "            if self.decoder is not None:\n",
        "                outputs = self.decoder(outputs, dtype=outputs.type())\n",
        "            outputs = postprocess(\n",
        "                outputs, self.num_classes, self.confthre, self.nmsthre\n",
        "            )\n",
        "            #logger.info(\"Infer time: {:.4f}s\".format(time.time() - t0))\n",
        "        return outputs, img_info\n",
        "\n",
        "\n",
        "def image_demo(predictor, vis_folder, current_time, args):\n",
        "    if osp.isdir(args.path):\n",
        "        files = get_image_list(args.path)\n",
        "    else:\n",
        "        files = [args.path]\n",
        "    files.sort()\n",
        "    tracker = BYTETracker(args, frame_rate=args.fps)\n",
        "    timer = Timer()\n",
        "    results = []\n",
        "\n",
        "    for frame_id, img_path in enumerate(files, 1):\n",
        "        outputs, img_info = predictor.inference(img_path, timer)\n",
        "        if outputs[0] is not None:\n",
        "            online_targets = tracker.update(outputs[0], [img_info['height'], img_info['width']], exp.test_size)\n",
        "            online_tlwhs = []\n",
        "            online_ids = []\n",
        "            online_scores = []\n",
        "            online_ages = []\n",
        "            online_genders = []\n",
        "            for t in online_targets:\n",
        "                tlwh = t.tlwh\n",
        "                tid = t.track_id\n",
        "                vertical = tlwh[2] / tlwh[3] > args.aspect_ratio_thresh\n",
        "                if tlwh[2] * tlwh[3] > args.min_box_area and not vertical:\n",
        "                    online_tlwhs.append(tlwh)\n",
        "                    online_ids.append(tid)\n",
        "                    online_scores.append(t.score)\n",
        "                    # save results\n",
        "                    results.append(\n",
        "                        f\"{frame_id},{tid},{tlwh[0]:.2f},{tlwh[1]:.2f},{tlwh[2]:.2f},{tlwh[3]:.2f},{t.score:.2f},-1,-1,-1\\n\"\n",
        "                    )\n",
        "            timer.toc()\n",
        "            online_im = plot_tracking(\n",
        "                img_info['raw_img'], online_tlwhs, online_ids, frame_id=frame_id, fps=1. / timer.average_time\n",
        "            )\n",
        "        else:\n",
        "            timer.toc()\n",
        "            online_im = img_info['raw_img']\n",
        "\n",
        "        # result_image = predictor.visual(outputs[0], img_info, predictor.confthre)\n",
        "        if args.save_result:\n",
        "            timestamp = time.strftime(\"%Y_%m_%d_%H_%M_%S\", current_time)\n",
        "            save_folder = osp.join(vis_folder, timestamp)\n",
        "            os.makedirs(save_folder, exist_ok=True)\n",
        "            cv2.imwrite(osp.join(save_folder, osp.basename(img_path)), online_im)\n",
        "\n",
        "        if frame_id % 20 == 0:\n",
        "            logger.info('Processing frame {} ({:.2f} fps)'.format(frame_id, 1. / max(1e-5, timer.average_time)))\n",
        "\n",
        "        ch = cv2.waitKey(0)\n",
        "        if ch == 27 or ch == ord(\"q\") or ch == ord(\"Q\"):\n",
        "            break\n",
        "\n",
        "    if args.save_result:\n",
        "        res_file = osp.join(vis_folder, f\"{timestamp}.txt\")\n",
        "        with open(res_file, 'w') as f:\n",
        "            f.writelines(results)\n",
        "        logger.info(f\"save results to {res_file}\")\n",
        "\n",
        "\n",
        "def imageflow_demo(predictor, vis_folder, current_time, args):\n",
        "    model = torch.jit.load('/content/ByteTrack/NewAttVGG.pt')\n",
        "    model.eval()\n",
        "    cap = cv2.VideoCapture(args.path if args.demo == \"video\" else args.camid)\n",
        "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)  # float\n",
        "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    timestamp = time.strftime(\"%Y_%m_%d_%H_%M_%S\", current_time)\n",
        "    save_folder = osp.join(vis_folder, timestamp)\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "    if args.demo == \"video\":\n",
        "        save_path = osp.join(save_folder, args.path.split(\"/\")[-1])\n",
        "    else:\n",
        "        save_path = osp.join(save_folder, \"camera.mp4\")\n",
        "    logger.info(f\"video save_path is {save_path}\")\n",
        "    vid_writer = cv2.VideoWriter(\n",
        "        save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (int(width), int(height))\n",
        "    )\n",
        "    tracker = BYTETracker(args, frame_rate=30)\n",
        "    timer = Timer()\n",
        "    frame_id = 0\n",
        "    results = []\n",
        "\n",
        "    tid_age_info_dict = defaultdict(list)\n",
        "    tid_gender_info_dict = defaultdict(list)\n",
        "    frame_history_age = 20\n",
        "    frame_history_gender = 20\n",
        "    pbar = tqdm(total=None, position=0, leave=True)\n",
        "    while True:\n",
        "        # if frame_id % 20 == 0:\n",
        "        #     logger.info('Processing frame {} ({:.2f} fps)'.format(frame_id, 1. / max(1e-5, timer.average_time)))\n",
        "        ret_val, frame = cap.read()\n",
        "        if ret_val:\n",
        "            outputs, img_info = predictor.inference(frame, timer)\n",
        "            if outputs[0] is not None:\n",
        "                online_targets = tracker.update(outputs[0], [img_info['height'], img_info['width']], exp.test_size)\n",
        "                online_tlwhs = []\n",
        "                online_ids = []\n",
        "                online_scores = []\n",
        "                online_ages = []\n",
        "                online_genders = []\n",
        "                for t in online_targets:\n",
        "                    tlwh = t.tlwh\n",
        "                    tid = t.track_id\n",
        "                    vertical = tlwh[2] / tlwh[3] > args.aspect_ratio_thresh\n",
        "                    if tlwh[2] * tlwh[3] > args.min_box_area and not vertical:\n",
        "                          online_tlwhs.append(tlwh)\n",
        "                          online_ids.append(tid)\n",
        "                          online_scores.append(t.score)\n",
        "                          if tlwh[0] < 0:\n",
        "                                tlwh[2] += tlwh[0]\n",
        "                                tlwh[0] = 0\n",
        "                          if tlwh[1] < 0:\n",
        "                              tlwh[3] += tlwh[1]\n",
        "                              tlwh[1]\n",
        "\n",
        "                          x, y, width, height = tlwh.astype(int)\n",
        "                          img = frame[y:y+height,x:x+width] / 255.0\n",
        "\n",
        "\n",
        "                          img = img.astype(np.float32)\n",
        "\n",
        "                          # print(img.shape)\n",
        "                          img = cv2.resize(img, (112, 224))\n",
        "                          # print(img.shape)\n",
        "                          # cv2.imwrite('img.jpg', img)\n",
        "\n",
        "                          xb = transforms.ToTensor()(img).unsqueeze(0).to('cuda')\n",
        "                          res,_,_= model(xb)\n",
        "                          # probs = torch.nn.functional.softmax(yb[0], dim=0)\n",
        "\n",
        "                          # res = model_age_gender.predict(img, verbose = False)\n",
        "                          # # print(res)\n",
        "                          res = res.cpu().detach().numpy()\n",
        "                          age = np.argmax(res[0][1:])\n",
        "                          gender =  1 if res[0][0] > 0.5 else 0\n",
        "                          # print('age',age)\n",
        "                          # print(gender)\n",
        "\n",
        "                          # LÆ°u giÃ¡ trá» age vÃ  gender vÃ o tá»« Äiá»n theo tid\n",
        "                          tid_age_info_dict[tid].append(age)\n",
        "                          tid_gender_info_dict[tid].append(gender)\n",
        "\n",
        "                          # Giá»¯ tá»« Äiá»n cÃ³ Ã­t hÆ¡n frame_history entries\n",
        "                          if len(tid_age_info_dict[tid]) > frame_history_age:\n",
        "                            tid_age_info_dict[tid].pop(0)\n",
        "                          if len(tid_gender_info_dict[tid]) > frame_history_gender:\n",
        "                            tid_gender_info_dict[tid].pop(0)\n",
        "\n",
        "                          # Láº¥y giÃ¡ trá» xuáº¥t hiá»n nhiá»u nháº¥t trong 20 frame\n",
        "                          most_common_age = max(tid_age_info_dict[tid], key=tid_age_info_dict[tid].count)\n",
        "                          most_common_gender = max(tid_gender_info_dict[tid], key=tid_gender_info_dict[tid].count)\n",
        "                          online_ages.append(most_common_age)\n",
        "                          online_genders.append(most_common_gender)\n",
        "                          results.append(\n",
        "                              f\"{frame_id+1},{tid},{tlwh[0]:.2f},{tlwh[1]:.2f},{tlwh[2]:.2f},{tlwh[3]:.2f},{most_common_gender},{most_common_age}\\n\"\n",
        "                          )\n",
        "                timer.toc()\n",
        "                online_im = plot_tracking(\n",
        "                    image=img_info['raw_img'],\n",
        "                    tlwhs=online_tlwhs,\n",
        "                    obj_ids=online_ids,\n",
        "                    ages=online_ages,\n",
        "                    genders=online_genders,\n",
        "                    frame_ids= frame_id + 1,\n",
        "                    fps=1. / timer.average_time\n",
        "                    )\n",
        "            else:\n",
        "                timer.toc()\n",
        "                online_im = img_info['raw_img']\n",
        "            if args.save_result:\n",
        "                vid_writer.write(online_im)\n",
        "            ch = cv2.waitKey(1)\n",
        "            if ch == 27 or ch == ord(\"q\") or ch == ord(\"Q\"):\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "        frame_id += 1\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "\n",
        "    if args.save_result:\n",
        "        res_file = osp.join(vis_folder, f\"{timestamp}.txt\")\n",
        "        with open(res_file, 'w') as f:\n",
        "            f.writelines(results)\n",
        "        logger.info(f\"save results to {res_file}\")\n",
        "\n",
        "\n",
        "def main(exp, args):\n",
        "    if not args.experiment_name:\n",
        "        args.experiment_name = exp.exp_name\n",
        "\n",
        "    output_dir = osp.join(exp.output_dir, args.experiment_name)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    if args.save_result:\n",
        "        vis_folder = osp.join(output_dir, \"track_vis\")\n",
        "        os.makedirs(vis_folder, exist_ok=True)\n",
        "\n",
        "    if args.trt:\n",
        "        args.device = \"gpu\"\n",
        "    args.device = torch.device(\"cuda\" if args.device == \"gpu\" else \"cpu\")\n",
        "\n",
        "    logger.info(\"Args: {}\".format(args))\n",
        "\n",
        "    if args.conf is not None:\n",
        "        exp.test_conf = args.conf\n",
        "    if args.nms is not None:\n",
        "        exp.nmsthre = args.nms\n",
        "    if args.tsize is not None:\n",
        "        exp.test_size = (args.tsize, args.tsize)\n",
        "\n",
        "    model = exp.get_model().to(args.device)\n",
        "    logger.info(\"Model Summary: {}\".format(get_model_info(model, exp.test_size)))\n",
        "    model.eval()\n",
        "\n",
        "    if not args.trt:\n",
        "        if args.ckpt is None:\n",
        "            ckpt_file = osp.join(output_dir, \"best_ckpt.pth.tar\")\n",
        "        else:\n",
        "            ckpt_file = args.ckpt\n",
        "        logger.info(\"loading checkpoint\")\n",
        "        ckpt = torch.load(ckpt_file, map_location=\"cpu\")\n",
        "        # load the model state dict\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        logger.info(\"loaded checkpoint done.\")\n",
        "\n",
        "    if args.fuse:\n",
        "        logger.info(\"\\tFusing model...\")\n",
        "        model = fuse_model(model)\n",
        "\n",
        "    if args.fp16:\n",
        "        model = model.half()  # to FP16\n",
        "\n",
        "    if args.trt:\n",
        "        assert not args.fuse, \"TensorRT model is not support model fusing!\"\n",
        "        trt_file = osp.join(output_dir, \"model_trt.pth\")\n",
        "        assert osp.exists(\n",
        "            trt_file\n",
        "        ), \"TensorRT model is not found!\\n Run python3 tools/trt.py first!\"\n",
        "        model.head.decode_in_inference = False\n",
        "        decoder = model.head.decode_outputs\n",
        "        logger.info(\"Using TensorRT to inference\")\n",
        "    else:\n",
        "        trt_file = None\n",
        "        decoder = None\n",
        "\n",
        "    predictor = Predictor(model, exp, trt_file, decoder, args.device, args.fp16)\n",
        "    current_time = time.localtime()\n",
        "    if args.demo == \"image\":\n",
        "        image_demo(predictor, vis_folder, current_time, args)\n",
        "    elif args.demo == \"video\" or args.demo == \"webcam\":\n",
        "        imageflow_demo(predictor, vis_folder, current_time, args)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = make_parser().parse_args()\n",
        "\n",
        "    args.path = '/content/videos/Kabu_0.mp4'\n",
        "    args.demo = 'video'\n",
        "\n",
        "    # args.exp_file = 'exps/example/mot/yolox_x_mix_mot20_ch.py'\n",
        "    # args.ckpt = '/content/ByteTrack/bytetrack_x_mot20.tar'\n",
        "    args.exp_file = 'exps/example/mot/yolox_x_mix_det.py'\n",
        "    args.ckpt = '/content/ByteTrack/bytetrack_x_mot17.pth.tar'\n",
        "\n",
        "    args.fp16 = True\n",
        "    args.fuse = True\n",
        "    args.save_result = True\n",
        "    args.mot20 = True\n",
        "\n",
        "    exp = get_exp(args.exp_file, args.name)\n",
        "\n",
        "    main(exp, args)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
